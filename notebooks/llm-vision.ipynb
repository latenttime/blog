{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘€ How does a LLM see?\n",
    "\n",
    "This notebook goes along side the blog post on [LLM vision](https://latenttime.com/blog/llm-vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers Pillow requests matplotlib seaborn scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/latenttime/blog/blob/main/public/static/images/lionfish.jpg?raw=true'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pipeline resizes the input image to the size expected by the model - which is 224x224. Let's do this manually\n",
    "# before digging further under the hood.\n",
    "\n",
    "image = image.resize((224, 224))\n",
    "# Display the image using matplotlib\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide the axes\n",
    "\n",
    "# Save the image using matplotlib\n",
    "plt.savefig('lionfish_resized.png', format='png', dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the patches\n",
    "patch_size = 64  # Size of the patches\n",
    "num_patches_x = image.size[0] // patch_size\n",
    "num_patches_y = image.size[1] // patch_size\n",
    "margin = 3  # Space between patches\n",
    "\n",
    "# Create a blank canvas with spaces between patches\n",
    "canvas_width = num_patches_x * (patch_size + margin) - margin\n",
    "canvas_height = num_patches_y * (patch_size + margin) - margin\n",
    "canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))\n",
    "\n",
    "# Extract and place each patch onto the canvas\n",
    "for i in range(num_patches_y):\n",
    "    for j in range(num_patches_x):\n",
    "        patch = image.crop((j * patch_size, i * patch_size, (j + 1) * patch_size, (i + 1) * patch_size))\n",
    "        canvas.paste(patch, (j * (patch_size + margin), i * (patch_size + margin)))\n",
    "\n",
    "# Display the image with patches\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(canvas)\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.savefig('patch_grid.png', format='png', dpi=300, bbox_inches='tight', pad_inches=0)  # Save as PNG with high resolution\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Visualization of the patches\n",
    "patch_size = 64  # Size of the patches\n",
    "num_patches_x = image.size[0] // patch_size\n",
    "num_patches_y = image.size[1] // patch_size\n",
    "margin = 3  # Space between patches\n",
    "\n",
    "# Create a blank canvas with spaces between patches\n",
    "canvas_width = num_patches_x * (patch_size + margin) - margin\n",
    "canvas_height = num_patches_y * (patch_size + margin) - margin\n",
    "canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))\n",
    "\n",
    "# Extract patches\n",
    "patches = []\n",
    "for i in range(num_patches_y):\n",
    "    for j in range(num_patches_x):\n",
    "        patch = image.crop((j * patch_size, i * patch_size, (j + 1) * patch_size, (i + 1) * patch_size))\n",
    "        patches.append(patch)\n",
    "\n",
    "# Randomly shuffle the patches\n",
    "random.shuffle(patches)\n",
    "\n",
    "# Place shuffled patches onto the canvas\n",
    "for idx, patch in enumerate(patches):\n",
    "    row = idx // num_patches_x\n",
    "    col = idx % num_patches_x\n",
    "    canvas.paste(patch, (col * (patch_size + margin), row * (patch_size + margin)))\n",
    "\n",
    "# Display the image with patches\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(canvas)\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.savefig('patch_grid_reordered.png', format='png', dpi=300, bbox_inches='tight', pad_inches=0)  # Save as PNG with high resolution\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'image' is already loaded somewhere in your code\n",
    "# Visualization of the patches\n",
    "num_patches_x = image.size[0] // patch_size\n",
    "num_patches_y = image.size[1] // patch_size\n",
    "margin = 3  # Space between patches\n",
    "\n",
    "# Instead of creating a canvas for a grid, create a long horizontal canvas\n",
    "total_patches = num_patches_x * num_patches_y\n",
    "canvas_width = total_patches * (patch_size + margin) - margin\n",
    "canvas_height = patch_size  # Only one row of patches\n",
    "canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))\n",
    "\n",
    "# Create an ImageDraw object to draw text\n",
    "draw = ImageDraw.Draw(canvas)\n",
    "\n",
    "# Extract and place each patch onto the canvas and add a number in a single line\n",
    "patch_number = 0\n",
    "for i in range(num_patches_y):\n",
    "    for j in range(num_patches_x):\n",
    "        patch = image.crop((j * patch_size, i * patch_size, (j + 1) * patch_size, (i + 1) * patch_size))\n",
    "        # Compute the position of the patch in the horizontal line\n",
    "        patch_x = patch_number * (patch_size + margin)\n",
    "        canvas.paste(patch, (patch_x, 0))\n",
    "        # Calculate the position for the text to be roughly in the center\n",
    "        patch_number += 1\n",
    "\n",
    "# Display the image with patches in a line\n",
    "plt.figure(figsize=(20, 2))  # Adjusted figure size for better view of the line\n",
    "plt.imshow(canvas)\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.savefig('patch_num_line.png', format='png', dpi=300, bbox_inches='tight', pad_inches=0)  # Save as PNG with high resolution\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a patch from somewhere in the middle\n",
    "middle_x = num_patches_x // 2\n",
    "middle_y = num_patches_y // 2\n",
    "patch = image.crop((middle_x * patch_size, middle_y * patch_size, \n",
    "                    (middle_x + 1) * patch_size, (middle_y + 1) * patch_size))\n",
    "\n",
    "# Display the selected patch\n",
    "plt.figure(figsize=(2, 2))  # Adjust figure size to better visualize the small patch\n",
    "plt.imshow(patch)\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.savefig('patch_single.png', format='png', dpi=300, bbox_inches='tight', pad_inches=0)  # Save as PNG with high resolution\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the patch to tensor and normalize\n",
    "patch_tensor = processor(images=patch, return_tensors=\"pt\").pixel_values\n",
    "patch_embedded = model.vit.embeddings.patch_embeddings(patch_tensor)\n",
    "print(patch_embedded.shape)\n",
    "print(patch_tensor.shape)\n",
    "\n",
    "# Batch size: 1\n",
    "# Image width/height: 224 (dictated by model)\n",
    "# Number of channels: 3 (RGB)\n",
    "# Patch size: 16x16\n",
    "# Number of patches: 196 (14 x 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These embeddings are subsequently used as the input sequence for the Transformer's encoder. Each embedding vector is treated similarly to a token embedding in NLP tasks, where each patch embedding represents part of the image information, analogous to how each word embedding would represent part of the semantic information in a text processing task. The model then processes these embeddings, considering both individual features and interactions between patches (through self-attention mechanisms), to understand and classify the image or perform other vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16 # Reset to the real patch size of the model\n",
    "\n",
    "img_width, img_height = image.size\n",
    "num_patches_x, num_patches_y = img_width // patch_size, img_height // patch_size\n",
    "\n",
    "print(num_patches_x, num_patches_y)\n",
    "\n",
    "patches = []\n",
    "for i in range(num_patches_y):\n",
    "    for j in range(num_patches_x):\n",
    "        # Extract patch\n",
    "        patch = image.crop((j * patch_size, i * patch_size, (j + 1) * patch_size, (i + 1) * patch_size))\n",
    "        patches.append(patch)\n",
    "\n",
    "# Process patch\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n",
    "# embeddings = outputs.hidden_states[-1][:, 0, :]  # Get the CLS token's embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.attentions[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_embeddings_for_layer(hidden_layer = 0):\n",
    "    patch_embeddings = []\n",
    "    # Iterate through the length of patches\n",
    "    for idx in range(len(patches)):\n",
    "        embeddings = outputs.hidden_states[hidden_layer][:, idx, :]  # Get the hidden state's embeddings\n",
    "        patch_embeddings.append(embeddings.squeeze(0))  # Squeeze to remove batch dimension and append\n",
    "    return patch_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install imageio pygifsicle --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from sklearn.decomposition import PCA\n",
    "import imageio\n",
    "\n",
    "def plot_patch_embeddings(patches, num_layers):\n",
    "    frames = []\n",
    "    for hidden_layer in range(num_layers):\n",
    "        patch_embeddings = get_patch_embeddings_for_layer(hidden_layer)\n",
    "        embeddings = torch.stack([emb.squeeze() for emb in patch_embeddings])\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        principal_components = pca.fit_transform(embeddings.detach().numpy())\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        for (x, y), patch in zip(principal_components, patches):\n",
    "            imgbox = OffsetImage(patch, zoom=0.5)\n",
    "            ab = AnnotationBbox(imgbox, (x, y), frameon=False, pad=0.1)\n",
    "            ax.add_artist(ab)\n",
    "\n",
    "        ax.update_datalim(principal_components)\n",
    "        ax.autoscale()\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.title(f'PCA of Patch Embeddings - Layer {hidden_layer}')\n",
    "\n",
    "        # Save the plot to a PNG file\n",
    "        filename = f'layer_{hidden_layer}.png'\n",
    "        plt.savefig(filename, format='png', dpi=300)\n",
    "        plt.close(fig)\n",
    "        frames.append(imageio.imread(filename))\n",
    "\n",
    "    # Create a GIF\n",
    "    imageio.mimsave('patch_embeddings_pca.gif', frames, duration=int(1000 * 1/1), loop=0)\n",
    "\n",
    "# Example usage\n",
    "# Assuming `model` is loaded and `patches` are prepared (images must be loaded as small patches)\n",
    "# and `num_layers` is the total number of layers in the model\n",
    "plot_patch_embeddings(patches, 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygifsicle import optimize\n",
    "optimize('patch_embeddings_pca.gif') # For overwriting the original one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
