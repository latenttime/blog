---
title: ðŸ‘€ How does a LLM see?
date: '2024-05-06'
tags: ['2024', 'llm', 'theory', 'cv', 'transformers']
draft: false
summary: Image encoding with Vision Transformers (ViT).
---

In 2024, many foundation models are released with native vision capabilities.
Anthropic Claude 3, OpenAI GPT-4V and Google Gemini all have the ability to generate a recipe from a photo of ingredients, or create a functional website from a doodle.

How does this work!? We don't yet know much about the architectures of the above models but we can look at vision encoders in some smaller and simpler classification models.
In this post we'll explore the initial encoding of images in a vision transformer model.

- [What is an Image?](#what-is-an-image)
- [Computer Images](#computer-images)
- [Introducing the Vision Transformer (ViT)](#introducing-the-vision-transformer-vit)
- [Hands on with the a ViT model](#hands-on-with-the-a-vit-model)
- [Visualising Patch Embeddings](#visualising-patch-embeddings)
- [Conclusion](#conclusion)

## What is an Image?

Let's start by reviewing at a very high level how biological vision works.
The human eye takes an input of electromagnetic waves in the visible spectrum that happen to have scattered off an object.
This light passes through the cornea and pupil, is refracted by the lens, focussed onto the retina.
The **image** is the pattern of light that is formed on the retina. It's a 2D projection of the 3D world.

> <Image alt="human eye" src="/static/images/human-eye.png" width={600} height={400} />
> <cite>Drawn by Stephen Hibbert on https://www.tldraw.com/</cite>

The retina is covered in photoreceptor cells which are sensitive to light.
The brain processes the electrical signal from the retina to create a mental model of the world around us.

## Computer Images

Computer vision works very differently to human vision but there are some analogies.
Instead of the biological eye, we have digital cameras with artificial sensors that convert light into electrical signals.
Instead of the brain, we have computer vision models that process these signals to understand the world around us.

The input to a vision model starts with a digital image.
A digital image is an 2D array of pixels where each pixel at coordinates `(x, y)` has a numerical value which is stored as bytes in a file.

Consider this image from the ImageNet dataset. It appears to be a picture of a lionfish:

> <Image alt="lionfish" src="/static/images/lionfish.jpg" width={346} height={346} />
> <cite>ImageNet dataset: https://image-net.org</cite>

The image has dimensions `640 x 480`. The pixel in position `(0, 320)` is in the first column and the middle row.
This pixel has the `(R, G, B)` value `(59, 142, 164)`. Each number is a byte with a value between 0 and 2<sup>8</sup> = 255.
Notice the highest value is in the blue channel, followed by green with
only a small contribution from red. This pixel shows the blue-green (turquoise) of the ocean:

> <div style={{ width: '100px', height: '100px', backgroundColor: 'rgb(59, 142, 164)' }}></div>
> <cite>Pixel (0, 320)</cite>

There are `w x h` pixels which makes `640 x 480 = 307,200` pixels for our lionfish image.
With the 3 channels, this means there are `307,200 x 3 = 921,600` bytes (almost 1MB) required to store the raw image.
However, this image is actually encoded as a JPEG file which uses a lossy compression algorithm to reduce the number of bytes
required to store on disk to just 49KB. But that's a topic for another time.

When feeding images to a computer vision model, we resize and 'flatten' it to map the pixel values into a 1D array
which is fed into the first layer of our neural network.

## Introducing the Vision Transformer (ViT)

A naive approach to processing images with a transformer would be to flatten the entire image into a 1D array of pixels.
The (normalised) raw pixel values would be passed to the model as input. For our `640 x 480` image, this would be a vector of length `640 x 480 x 3 = 921,600`.
However, we know that the pixel values are not independent of each other. The pixel at position `(0, 0)` is likely to be similar to the pixel at position `(1, 0)`.
Furthermore, there are sequences of pixels that form lines, shapes and semantics to the image.
These inner representations in the image which can be learned by a transformer vision model at training time.
The dimensionality of the input is much higher than the actual information content of the image for real world distributions of images.

On a more practical level, we know that the attention blocks in the transformer architectures scale quadratically with the input length.
Therefore it would be very computationally expensive to process raw images in this way.

In practice, we split an image into fixed-size patches (e.g. 16x16 pixels), linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard transformer encoder.
This approach was laid out in [An Image is Worth 16x16 Words](https://arxiv.org/pdf/2010.11929), showing how transformers can be applied to computer vision.

## Hands on with the a ViT model

We'll use a [Google vision transformer model](https://huggingface.co/google/vit-base-patch16-224) downloaded from Hugging Face to process our lionfish image and extract the features so we can build an intuition for what's happening.

```python
from transformers import ViTImageProcessor, ViTForImageClassification
from PIL import Image
import requests
import matplotlib.pyplot as plt


processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')
```

Now load the lionfish image as a PIL image and then we can pass it to the model.
We need to resize to the size expected by the model - which is 224x224. Let's do this manually before digging further under the hood.

```python
url = 'https://github.com/latenttime/blog/blob/main/public/static/images/lionfish.jpg?raw=true'
image = Image.open(requests.get(url, stream=True).raw)
image = image.resize((224, 224))
image
```

We can see the resulting squished and more pixelated image below:

<Image alt="lionfish resized" src="/static/images/lionfish_resized.png" width={320} height={240} />

The true patch size of `google/vit-base-patch16-224` is 16x16 pixels.
In reality there are 14x14=196 patches for the 224x224 image.
To aid our visualisations we'll pretend the patch size is 64x64 pixels which makes 9 patches.
Bear in mind that the model is actually working with 4x smaller patches.

<Image alt="patch grid" src="/static/images/patch_grid.png" width={320} height={240} />

Let's choose a single patch and see how we go from RBG pixel integers to a 768 dimension embedding vector.
The first step is to flatten the patch. Similar to how we flatten the 2D array of patches in the original image, we take the pixels row by row and concatenate them into a 1D array.
We then normalise the integer RGB pixel values to be between 0 and 1.
It's then common to use a linear projection which is implemented as a learnable linear layer.
This multiplication by a weight matrix is equivalent to a linear transformation of the pixel values.
The output is a 768 dimension latent vector that no longer directly represents the pixel values but instead a learned representation of the patch.

<Image alt="patch flat" src="/static/images/patch_flat.png" width={640} height={240} />

Note that the patch sequence order matters here. Just like with text, the meaning changes if the patches are shuffled.

<Image
  alt="patch reordered grid"
  src="/static/images/patch_grid_reordered.png"
  width={320}
  height={240}
/>

In order to represent the correct order [position embeddings are added](https://github.com/google-research/vision_transformer/blob/143bd26cf285f835a2d1954f85b14f33f7d3ea8e/vit_jax/models_vit.py#L37-L63) to the image patch embeddings.
A [class token (CLS) is optionally added](https://github.com/google-research/vision_transformer/blob/143bd26cf285f835a2d1954f85b14f33f7d3ea8e/vit_jax/models_vit.py#L279-L283) for classification tasks.
The encoded image embeddings are subsequently input to the transformer block in the LLM.

<Image alt="patch embedding" src="/static/images/patch_embedding.png" width={640} height={240} />

Finally, we have our image patch tokens, which are the input to the transformer encoder.

## Visualising Patch Embeddings

We can extract the patch embeddings from the model's hidden states to visualise the information the model is working with.
With the transformers library we can do this by setting `output_hidden_states=True` in the forward pass.

```python
def get_patch_embeddings_for_layer(hidden_layer = 0):
    patch_embeddings = []
    # Iterate through the length of patches
    for idx in range(len(patches)):
        embeddings = outputs.hidden_states[hidden_layer][:, idx, :]  # Get the hidden state's embeddings
        patch_embeddings.append(embeddings.squeeze(0))  # Squeeze to remove batch dimension and append
    return patch_embeddings

inputs = processor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs, output_hidden_states=True)
    patch_embeddings = get_patch_embeddings_for_layer(hidden_layer)
    embeddings = torch.stack([emb.squeeze() for emb in patch_embeddings])
```

Each embedding vector is treated similarly to a token embedding in NLP tasks, where each patch embedding represents part of the image information, analogous to how each word embedding would represent part of the semantic information in a text processing task.
We can visualise the 768 dimension embedding vectors in a 2D space using PCA to reduce the dimensionality of the embeddings.
Below we see the patch embeddings for the first layer of the transformer.
Note we switched back to the true 16x16 patch size for this visualisation.

<Image alt="patch embeddings layer 0" src="/static/images/layer_0.png" width={640} height={640} />

The transformer then processes these embeddings, considering both individual features and interactions between patches (through self-attention mechanisms), to understand and classify the image or perform other vision tasks.

The `google/vit-base-patch16-224` model has 12 hidden layers in total.
Below we can see how the PCA of the embeddings evolve through each layer in the transformer in this model which is trained on ImageNet.
The vector output from the final layer is used to predict the class of the image.
Note that in an vision capable LLM trained on a different task, the activations will be different but the principle remains the same.

<Image
  alt="patch embeddings gif"
  src="/static/images/patch_embeddings_pca.gif"
  width={640}
  height={640}
/>

We can see in the [model card](https://huggingface.co/google/vit-base-patch16-224) that this model has been trained for zero shot image classification on the ImageNet dataset.
There is a prediction processing head that takes the output of the transformer blocks to maps the final hidden state to the 1,000 ImageNet classes.

So our lionfish was actually in the training set so hopefully the model can predict the correct class:

```python
inputs = processor(images=image, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
# model predicts one of the 1000 ImageNet classes
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx])
```

```
Predicted class: lionfish
```

Indeed it does!

## Conclusion

We have learned about vision transformers (likely similar to those used in modern LLMs vision encoders).
We saw how images are tokenised splitting them into patches, linearly projecting them and adding position embeddings.
We got a glimpse of how this representation allows the model to understand and classify images, and perform other tasks performed by vision capable LLMs.

You can find the code used to create the visualisations for this post in this [notebook](https://github.com/latenttime/blog/blob/main/notebooks/llm-vision.ipynb).

Subscribe to be notified for the next post, where we will look at how we can evaluate the visual acuity of some popular vision LLMs.

Thanks for reading, please comment, tweet or email me with any questions or feedback.
